{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from numpy.random import seed\n",
    "seed(1)\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from analysis import *\n",
    "from collections import namedtuple\n",
    "import Metrics\n",
    "from PatientSet import PatientSet\n",
    "from Constants import Constants\n",
    "from dependencies.Boruta import BorutaPy\n",
    "\n",
    "#for getting the fisher exact test\n",
    "import rpy2.robjects.numpy2ri\n",
    "from rpy2.robjects.packages import importr\n",
    "rpy2.robjects.numpy2ri.activate()\n",
    "\n",
    "#sklearn dependencies\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import recall_score, roc_auc_score, f1_score\n",
    "from sklearn.ensemble import ExtraTreesClassifier, RandomForestClassifier\n",
    "from sklearn.utils import resample\n",
    "from scipy.cluster.hierarchy import fcluster, linkage\n",
    "\n",
    "#we get like a million deprication errors for some reason with the external libraries\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'l1' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-a50d1d669727>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#class wrapper with for heirarchical clustering with more linkages than sklearn\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[1;32mclass\u001b[0m \u001b[0mFClusterer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_clusters\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdist_func\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0ml1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlink\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'weighted'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcriterion\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'maxclust'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlink\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlink\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-2-a50d1d669727>\u001b[0m in \u001b[0;36mFClusterer\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0mFClusterer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m     \u001b[1;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_clusters\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdist_func\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0ml1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlink\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'weighted'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcriterion\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'maxclust'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlink\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlink\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdist_func\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdist_func\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mlink\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m'median'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'ward'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'centroid'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;34m'euclidean'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'l1' is not defined"
     ]
    }
   ],
   "source": [
    "#class wrapper with for heirarchical clustering with more linkages than sklearn\n",
    "class FClusterer():\n",
    "    \n",
    "    def __init__(self, n_clusters, dist_func = l1, link = 'weighted', criterion = 'maxclust'):\n",
    "        self.link = link\n",
    "        self.dist_func = dist_func if link not in ['median', 'ward', 'centroid'] else 'euclidean'\n",
    "        self.t = n_clusters\n",
    "        self.criterion = criterion\n",
    "\n",
    "    def fit_predict(self, x, y = None):\n",
    "        clusters = linkage(x, method = self.link, metric = self.dist_func)\n",
    "        return fcluster(clusters, self.t, criterion = self.criterion)\n",
    "\n",
    "def l1(x1, x2):\n",
    "    return np.sum(np.abs(x1-x2))\n",
    "\n",
    "def tanimoto_dist(x1, x2):\n",
    "    if l1(x1 - x2) == 0:\n",
    "        return 0\n",
    "    tanimoto = x1.dot(x2)/(x1.dot(x1) + x2.dot(x2) - x1.dot(x2))\n",
    "    return 1/(1+tanimoto)\n",
    "\n",
    "def l2(x1, x2):\n",
    "    return np.sqrt(np.sum((x1-x2)**2))\n",
    "\n",
    "def pdist(x, dist_func):\n",
    "    distance = []\n",
    "    for i in range(x.shape[0]):\n",
    "        for j in range(x.shape[0]):\n",
    "            distance.append(dist_func(x[i], x[j]))\n",
    "    return np.array(distance)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_result = namedtuple('cluster_result', ['method', 'cluster', 'correlation', 'model'])\n",
    "\n",
    "def get_clusterers(ks = [2,5]):\n",
    "    c_range = range(ks[0], ks[1])\n",
    "    clusterers = {}\n",
    "    clusterers['l1_weighted'] = [FClusterer(c) for c in c_range]\n",
    "    clusterers['l2_weighted'] = [FClusterer(c, dist_func = l2) for c in c_range]\n",
    "    clusterers['l1_complete'] = [FClusterer(c, link = 'complete') for c in c_range]\n",
    "    clusterers['l2_complete'] = [FClusterer(c, dist_func = l2, link = 'complete') for c in c_range]\n",
    "    clusterers['centroid'] = [FClusterer(c, link='centroid') for c in c_range]\n",
    "    clusterers['median'] = [FClusterer(c, link = 'median') for c in c_range]\n",
    "    clusterers['ward'] = [FClusterer(c, link='ward') for c in c_range]\n",
    "    return clusterers\n",
    "\n",
    "def fisher_exact_test(c_labels, y):\n",
    "    if len(set(y)) == 1:\n",
    "        print('fisher test run with no positive class')\n",
    "        return 0\n",
    "#        assert(len(set(y)) == 2)\n",
    "    #call fishers test from r\n",
    "    contingency = get_contingency_table(c_labels, y)\n",
    "    stats = importr('stats')\n",
    "    pval = stats.fisher_test(contingency)[0][0]\n",
    "    return pval\n",
    "\n",
    "def get_contingency_table(x, y):\n",
    "    #assumes x and y are two equal length vectors, creates a mxn contigency table from them\n",
    "    cols = sorted(list(np.unique(y)))\n",
    "    rows = sorted(list(np.unique(x)))\n",
    "    tabel = np.zeros((len(rows), len(cols)))\n",
    "    for row_index in range(len(rows)):\n",
    "        row_var = rows[row_index]\n",
    "        for col_index in range(len(cols)):\n",
    "            rowset = set(np.argwhere(x == row_var).ravel())\n",
    "            colset = set(np.argwhere(y == cols[col_index]).ravel())\n",
    "            tabel[row_index, col_index] = len(rowset & colset)\n",
    "    return tabel\n",
    "\n",
    "def analyze_clusters(target_var, name, clusterer, features):\n",
    "    result = []\n",
    "    clusters = clusterer.fit_predict(features).ravel()\n",
    "    n_clusters = len(set(clusters))\n",
    "    if n_clusters < 2:\n",
    "        return None\n",
    "    method = name + str(n_clusters)\n",
    "\n",
    "    overall_correlation = fisher_exact_test(clusters, target_var)\n",
    "    result.append( cluster_result(method, 'all',\n",
    "                                  overall_correlation,\n",
    "                                  clusterer))\n",
    "\n",
    "    for c in np.unique(clusters):\n",
    "        correlation = fisher_exact_test(clusters == c, target_var)\n",
    "        result.append( cluster_result(method, str(c+1),\n",
    "                                      correlation, clusterer))\n",
    "    return result\n",
    "\n",
    "def cluster(target_var, features, args = None):\n",
    "    if args is not None:\n",
    "        assert( isinstance(args, list) )\n",
    "        features = features[:, args]\n",
    "    results = []\n",
    "    clusterers = get_clusterers()\n",
    "    for cname, clusterers in clusterers.items():\n",
    "        for clusterer in clusterers:\n",
    "            analysis = analyze_clusters(target_var, cname, clusterer, features)\n",
    "            if analysis is not None:\n",
    "                results.extend(analysis)\n",
    "    results = sorted(results, key = lambda x: x.correlation)\n",
    "    return results\n",
    "\n",
    "def get_optimal_clustering(features, target_var, args = None, patient_subset = None):\n",
    "    clusters = np.zeros(target_var.shape)\n",
    "    if patient_subset is not None:\n",
    "        target = target_var[patient_subset]\n",
    "        features = features[patient_subset,:]\n",
    "    else:\n",
    "        target = target_var\n",
    "    result = cluster(target, features, args)\n",
    "    result = [r for r in result if r.cluster is 'all']\n",
    "    if args is not None:\n",
    "        features = features[:, args]\n",
    "    clusters[patient_subset] = result[0].model.fit_predict(features).ravel() + 1\n",
    "    pval = fisher_exact_test(clusters, target_var)\n",
    "    clusterer_data = cluster_result(method = result[0].method,\n",
    "                                    cluster = result[0].cluster,\n",
    "                                    correlation = pval,\n",
    "                                    model = result[0].model)\n",
    "    optimal = (clusters, clusterer_data)\n",
    "    return optimal\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#load in the patientset object that has all the patient info\n",
    "db = PatientSet()\n",
    "\n",
    "#add a bunch of features to the object that we'll want to try\n",
    "#so we can use the db.to_dataframe function to get them all in a nice dataframe with one-hot encoding and labels automatically\n",
    "db.discrete_dists = Metrics.discretize(-db.tumor_distances, n_bins = 15, strategy='uniform')\n",
    "db.t_volumes = np.array([np.sum([g.volume for g in gtvs]) for gtvs in db.gtvs]).reshape(-1,1)\n",
    "db.tsimdoses = tsim_prediction(db)\n",
    "db.toxicity = db.feeding_tubes + db.aspiration > 0\n",
    "db.xerostima = db.feeding_tubes + db.aspiration > 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#parameters for the experiments\n",
    "toxicities_to_test = ['feeding_tubes', 'aspiration', 'toxicity']\n",
    "\n",
    "#features to test the feature selection on.  should be fields in the patientset we have\n",
    "candidate_features = ['discrete_dists', 'volumes', 't_volumes', \n",
    "                      'lateralities']\n",
    "\n",
    "#number of times to resample and doing feature selection\n",
    "#if n = 1, just use the first result\n",
    "n_samples = 30\n",
    "\n",
    "#type of scaling to use\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "#put some bounds on the features to subset\n",
    "min_features = 2\n",
    "\n",
    "#class used to subset the data, default is what the original paper suggests, roughly\n",
    "boruta = BorutaPy(ExtraTreesClassifier(n_estimators = 500), n_estimators = 600)\n",
    "\n",
    "#where to save results, put None if you don't want to save\n",
    "save_root = 'data/clustering_results/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#our actual experiment, try to find correlations using the boruta method and such\n",
    "feature_list = []\n",
    "for tox_name in toxicities_to_test:\n",
    "    print(tox_name)\n",
    "    toxicity = getattr(db, tox_name) > 0\n",
    "    \n",
    "    #use actual doses to train, and predicted for the clustering \n",
    "    train = db.to_dataframe(candidate_features + ['doses'])\n",
    "    test = db.to_dataframe(candidate_features + ['tsimdoses'])\n",
    "    \n",
    "    #we're going to resample the data, scale it, and apply the boruta method n_sample times\n",
    "    def get_resampled_support(x, y):\n",
    "        if n_samples > 1:\n",
    "            x, y = resample(x.values, y)\n",
    "        x = scaler.fit_transform(x)\n",
    "        boruta.fit(x, y)\n",
    "        return boruta.support_, boruta.support_weak_\n",
    "    \n",
    "    #save the boruta support for each trial in a dataframe, scores are % of time the variable has support or weak support\n",
    "    supports = pd.DataFrame(data = np.zeros((2,train.shape[1])), columns = test.columns, index =['support', 'weak_support'])\n",
    "    for n in range(n_samples):\n",
    "        sup, weak_sup = get_resampled_support(train, toxicity)\n",
    "        supports.loc['support'] += sup/n_samples\n",
    "        supports.loc['weak_support'] += weak_sup/n_samples\n",
    "        \n",
    "    #try out a bunch of thresholds on how good the variable is supported vs cluster results\n",
    "    best_correlation = 1\n",
    "    prev_argcount = test.shape[1]\n",
    "    for support_thresh in [.2,.3,.4,.5,.6,.7,.8,.9]:\n",
    "        top_args = np.argwhere(supports.loc['support'] >= support_thresh).ravel()\n",
    "        if len(top_args) < min_features:\n",
    "            break\n",
    "        #check to see that we actually added more features\n",
    "        if len(top_args) == prev_argcount:\n",
    "            continue\n",
    "        prev_argcount = len(top_args)\n",
    "        to_use = test.iloc[:, top_args]\n",
    "        print('number of features: ', len(train.columns[top_args]))\n",
    "        \n",
    "        #we're going to try a bunch of different clusterings and look at the best result\n",
    "        clustering = get_optimal_clustering(scaler.fit_transform(to_use.values), toxicity)\n",
    "        print(clustering[1].method)\n",
    "        print(get_contingency_table(clustering[0], toxicity))\n",
    "        print('correlation: ', clustering[1].correlation,'\\n')\n",
    "        #save the feature set with the best (lowest) correlation\n",
    "        if clustering[1].correlation < best_correlation:\n",
    "            best_correlation = clustering[1].correlation\n",
    "            best_clusters = clustering[0]\n",
    "            best_features = copy(to_use)\n",
    "            n_best_clusters = len(set(clustering[0]))\n",
    "            \n",
    "    #check that we actually got a result\n",
    "    if best_correlation == 1:\n",
    "        print('no good values')\n",
    "        continue\n",
    "    best_features['cluster_labels'] = best_clusters\n",
    "    print(best_features.columns)\n",
    "    best_features.index.rename('Dummy.ID', inplace = True)\n",
    "    feature_list.append(best_features)\n",
    "    if save_root is not None:\n",
    "        best_features.to_csv(save_root\n",
    "                     + 'boruta_features_k='\n",
    "                     + str(n_best_clusters)\n",
    "                     + '_p=' + '{:.3e}'.format(best_correlation)\n",
    "                     + '_toxicity=' + tox_name + '.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get all the features found before and put them together\n",
    "combined_df = feature_list[0]\n",
    "for i in range(1, len(feature_list)):\n",
    "    df2 = feature_list[i]\n",
    "    to_drop = list(set(combined_df.columns).intersection(set(df2.columns)))\n",
    "    if len(to_drop) == df2.shape[1]:\n",
    "        continue\n",
    "    df2 = df2.drop(to_drop, axis = 1)\n",
    "    combined_df = pd.merge(combined_df, df2, on = 'Dummy.ID')\n",
    "combined_df.drop('cluster_labels', axis = 1, inplace = True)\n",
    "print(combined_df.columns)\n",
    "combined_clusters = get_optimal_clustering(scaler.fit_transform(combined_df.values), db.toxicity)\n",
    "print(combined_clusters[1].method)\n",
    "print(get_contingency_table(combined_clusters[0], toxicity))\n",
    "print('correlation: ', combined_clusters[1].correlation,'\\n')\n",
    "combined_df.cluster_labels = combined_clusters[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if save_root is not None:\n",
    "    best_features.to_csv(save_root\n",
    "                 + 'boruta_features_k='\n",
    "                 + str(len(set(combined_clusters[0])))\n",
    "                 + '_p=' + '{:.3e}'.format(combined_clusters[1].correlation)\n",
    "                 + '_toxicity=combinedToxicity.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
