{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0925 18:08:04.921000  3688 deprecation_wrapper.py:119] From F:\\Skool\\EVL_Research\\CAMP-RT\\PYTHON\\PatientSet.py:8: The name tf.set_random_seed is deprecated. Please use tf.compat.v1.set_random_seed instead.\n",
      "\n",
      "Using TensorFlow backend.\n",
      "F:\\Anaconda\\lib\\site-packages\\dask\\config.py:168: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.\n",
      "  data = yaml.load(f.read()) or {}\n",
      "F:\\Anaconda\\lib\\site-packages\\sklearn\\externals\\six.py:31: DeprecationWarning: The module is deprecated in version 0.21 and will be removed in version 0.23 since we've dropped support for Python 2.7. Please rely on the official version of six (https://pypi.org/project/six/).\n",
      "  \"(https://pypi.org/project/six/).\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "from numpy.random import seed\n",
    "seed(1)\n",
    "\n",
    "from PatientSet import PatientSet\n",
    "from Constants import Constants\n",
    "import Metrics\n",
    "from analysis import *\n",
    "from Models import *\n",
    "from copy import copy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from time import time\n",
    "from datetime import datetime\n",
    "\n",
    "from dependencies.NCA import NeighborhoodComponentsAnalysis\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "from scipy.special import softmax\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.preprocessing import OneHotEncoder, QuantileTransformer\n",
    "\n",
    "from sklearn.naive_bayes import BernoulliNB, ComplementNB, GaussianNB, MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "from sklearn.ensemble import VotingClassifier, ExtraTreesClassifier, RandomForestClassifier, AdaBoostClassifier\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "from imblearn import under_sampling, over_sampling, combine\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "\n",
    "baseline_feature_file = 'data/baselineClustering.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MetricLearningClassifier(BaseEstimator, ClassifierMixin):\n",
    "\n",
    "    def __init__(self, n_components = 'auto',\n",
    "                 random_state = 1,\n",
    "                 resampler = None,\n",
    "                 resample_after_nca = True,\n",
    "                 use_softmax = True):\n",
    "        self.n_components = n_components\n",
    "        if n_components is not 'auto':\n",
    "            self.transformer = NeighborhoodComponentsAnalysis(n_components = n_components)\n",
    "        self.group_parameters = namedtuple('group_parameters', ['means', 'inv_covariance', 'max_dist'])\n",
    "        self.resampler = resampler\n",
    "        self.use_softmax = use_softmax\n",
    "        self.resample_after_nca = resample_after_nca\n",
    "\n",
    "    def get_optimal_components(self, x, y):\n",
    "        n_components = x.shape[1]\n",
    "        def get_score():\n",
    "            nca = NeighborhoodComponentsAnalysis(n_components = n_components)\n",
    "            nca.fit(x,y)\n",
    "            return silhouette_score(nca.transform(x), y), nca\n",
    "        score, nca = get_score()\n",
    "        while True:\n",
    "            if n_components <= 2:\n",
    "                return nca\n",
    "            n_components -= 1\n",
    "            new_score, new_nca = get_score()\n",
    "            if new_score > 1.1*score:\n",
    "                score = new_score\n",
    "                nca = new_nca\n",
    "            else:\n",
    "                return nca\n",
    "            \n",
    "    def resample(self, x, y):\n",
    "        if self.resampler is not None:\n",
    "            xtemp, ytemp = self.resampler.fit_resample(x,y)\n",
    "            if len(np.unique(ytemp)) == len(np.unique(y)):\n",
    "                x = xtemp\n",
    "                y = ytemp\n",
    "        return x, y\n",
    "\n",
    "    def fit(self, x, y):\n",
    "        if not self.resample_after_nca:\n",
    "            x, y = self.resample(x,y)\n",
    "        if self.n_components == 'auto':\n",
    "            self.transformer = self.get_optimal_components(x, y)\n",
    "        self.transformer.fit(x, y)\n",
    "        self.groups = OrderedDict()\n",
    "        if self.resample_after_nca:\n",
    "            x, y = self.resample(x,y)\n",
    "        for group in np.unique(y):\n",
    "            self.groups[group] = self.group_params(x, y, group)\n",
    "\n",
    "    def group_params(self, x, y, group):\n",
    "        targets = np.argwhere(y == group).ravel()\n",
    "        x_target = self.transformer.transform(x[targets])\n",
    "        fmeans = x_target.mean(axis = 0)\n",
    "        inv_cov = np.linalg.pinv(np.cov(x_target.T))\n",
    "        train_dists = self.mahalanobis_distances(x, self.group_parameters(fmeans, inv_cov, 0))\n",
    "        parameters = self.group_parameters(fmeans, inv_cov, train_dists.max())\n",
    "        return parameters\n",
    "\n",
    "    def mahalanobis_distances(self, x, group):\n",
    "        x_offset = self.transformer.transform(x) - group.means\n",
    "        left_term = np.dot(x_offset, group.inv_covariance)\n",
    "        mahalanobis = np.dot(left_term, x_offset.T).diagonal()\n",
    "        return mahalanobis\n",
    "\n",
    "    def predict_proba(self, x):\n",
    "        all_distances = []\n",
    "        for group_id, group_params in self.groups.items():\n",
    "            distances = self.mahalanobis_distances(x, group_params)\n",
    "            proximity = np.clip(1 - (distances/group_params.max_dist), 0.00001, 1)\n",
    "            all_distances.append(proximity)\n",
    "        output = np.hstack(all_distances).reshape(-1, len(self.groups.keys()))\n",
    "        if self.use_softmax:\n",
    "            output = softmax(output)\n",
    "        else:\n",
    "            output = output/output.sum(axis = 1).reshape(-1,1)\n",
    "        return output\n",
    "\n",
    "    def predict(self, x):\n",
    "        labels = list(self.groups.keys())\n",
    "        probs = self.predict_proba(self, x)\n",
    "        max_probs =  np.argmax(probs, axis = 1).ravel()\n",
    "        ypred = np.zeros(max_probs.shape).astype(np.dtype(labels[0]))\n",
    "        for i in range(max_probs.shape[0]):\n",
    "            ypred[i] = labels[max_probs[i]]\n",
    "        return ypred[i]\n",
    "\n",
    "    def fit_predict(self, x, y):\n",
    "        self.fit(x,y)\n",
    "        return self.predict(x)\n",
    "\n",
    "class BayesWrapper(BaseEstimator, ClassifierMixin):\n",
    "\n",
    "    def __init__(self, bayes = BernoulliNB(alpha = 0), n_categories = None):\n",
    "        if n_categories is None:\n",
    "            self.encoder = OneHotEncoder(categories = 'auto',\n",
    "                                         sparse = False,\n",
    "                                         handle_unknown = 'ignore')\n",
    "        else:\n",
    "            self.encoder = KBinsDiscretizer(n_bins = n_categories, encode = 'ordinal')\n",
    "        self.bayes = bayes\n",
    "\n",
    "    def fit(self, x, y):\n",
    "        x = self.encoder.fit_transform(x)\n",
    "        self.bayes.fit(x,y)\n",
    "        return self\n",
    "\n",
    "    def predict(self, x):\n",
    "        xpred = self.encoder.transform(x)\n",
    "        return self.bayes.predict(xpred)\n",
    "\n",
    "    def predict_proba(self, x):\n",
    "        xpred = self.encoder.transform(x)\n",
    "        return self.bayes.predict_proba(xpred)\n",
    "\n",
    "    def fit_predict(self, x, y, probability = False):\n",
    "        self.fit(x,y)\n",
    "        if probability:\n",
    "            return self.predict_proba(x)\n",
    "        return self.predict(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "def presplit_roc_cv(classifier, data_split):\n",
    "    ypred = np.zeros((len(data_split),))\n",
    "    y = np.array([split['ytest'] for split in data_split])\n",
    "    i = 0\n",
    "    for split in data_split:\n",
    "        classifier.fit(split['xtrain'], split['ytrain'])\n",
    "        ypred[i] = classifier.predict_proba(split['xtest'])[0,1]\n",
    "        if i == 0:\n",
    "            has_importances = hasattr(classifier, 'feature_importances_')\n",
    "        if has_importances:\n",
    "            if i == 0:\n",
    "                importances = classifier.feature_importances_\n",
    "            else:\n",
    "                importances += classifier.feature_importances_\n",
    "        i += 1\n",
    "    if has_importances:\n",
    "        importances /= i\n",
    "        importances = pd.Series(data = importances, index = data_split[0]['feature_labels'])\n",
    "    else:\n",
    "        importances = None\n",
    "    return roc_auc_score(y, ypred), importances\n",
    "\n",
    "def discretize_continuous_fields(df, n_bins):\n",
    "    encoder = KBinsDiscretizer(n_bins = n_bins, encode = 'ordinal')\n",
    "    for col in df.columns:\n",
    "        vals = df[col].values\n",
    "        if len(np.unique(vals)) > n_bins:\n",
    "            df[col] = encoder.fit_transform(vals.reshape(-1,1)).ravel()\n",
    "    return df\n",
    "\n",
    "def get_all_splits(df, regularizer, outcomes):\n",
    "    data_splits = {}\n",
    "    for outcome in outcomes:\n",
    "        splits = {str(resampler): get_splits(df, outcome[0], regularizer, [resampler]) for resampler in resamplers}\n",
    "        data_splits[outcome[1]] = splits\n",
    "    return data_splits\n",
    "\n",
    "def get_splits(df, y, regularizer = None, resamplers = None):\n",
    "    x = df.values\n",
    "    feature_labels = list(df.columns)\n",
    "    loo = LeaveOneOut()\n",
    "    splits = []\n",
    "    for train, test in loo.split(x):\n",
    "        split = {}\n",
    "        xtrain, ytrain = x[train], y[train]\n",
    "        xtest, ytest = x[test], y[test]\n",
    "        if regularizer is not None:\n",
    "            xtrain = regularizer.fit_transform(xtrain)\n",
    "            xtest = regularizer.transform(xtest)\n",
    "        for resampler in resamplers:\n",
    "            if resampler is None:\n",
    "                continue\n",
    "            xtrain, ytrain = resampler.fit_resample(xtrain, ytrain)\n",
    "        split['xtrain'] = xtrain\n",
    "        split['xtest'] = xtest\n",
    "        split['ytrain'] = ytrain\n",
    "        split['ytest'] = ytest\n",
    "        split['train_index'] = train\n",
    "        split['test_index'] = test\n",
    "        split['feature_labels'] = feature_labels\n",
    "        splits.append(split)\n",
    "    return splits\n",
    "\n",
    "def cluster_features(db,\n",
    "                  baseline_features = baseline_feature_file,\n",
    "                  use_baseline_features = True,\n",
    "                  top_features = 'data/clustering_results/toxicityClustering.csv',\n",
    "                  use_top_features = True,\n",
    "                  discrete_features = False,\n",
    "                  cluster_names = ['kmeans_k=4','cluster_labels']):\n",
    "    baseline = pd.read_csv(baseline_features, index_col = 'Dummy.ID').drop('Unnamed: 0', axis = 1)\n",
    "    all_clusters = set(['manhattan_k=2','manhattan_k=3','manhattan_k=4',\n",
    "                        'cluster_labels','hc_ward2','hc_ward4',\n",
    "                        'FT','AR','TOX'])\n",
    "    non_features = list(all_clusters - set(cluster_names))\n",
    "    \n",
    "    if use_baseline_features:\n",
    "        cluster_names = cluster_names + list(baseline.drop(non_features, axis = 1, errors='ignore').columns)\n",
    "    if 'T.category' in cluster_names:\n",
    "        dist_clusters['T.category'] = dist_clusters['T.category'].apply(lambda x: int(x[1]))\n",
    "        \n",
    "    if isinstance(top_features, str):\n",
    "        dist_clusters = pd.read_csv(top_features, index_col = 0)\n",
    "        dist_clusters.index.rename('Dummy.ID', inplace = True)\n",
    "        if use_top_features:\n",
    "            cluster_names = cluster_names + list( dist_clusters.drop(non_features,axis=1, errors='ignore').columns)\n",
    "        df = baseline.merge(dist_clusters, on=['Dummy.ID'])\n",
    "    else:\n",
    "        df = baseline\n",
    "    ft = df.FT.values\n",
    "    ar = df.AR.values\n",
    "    tox = df.TOX.values\n",
    "    to_drop = set(df.columns) - set(cluster_names)\n",
    "    df = df.drop(to_drop, axis = 1, errors = 'ignore')\n",
    "    if discrete_features:\n",
    "        df = discretize_continuous_fields(df, 5)\n",
    "    columns = df.columns\n",
    "    for col in columns:\n",
    "        if col in all_clusters:\n",
    "            groups = set(df[col].values)\n",
    "            for g in groups:\n",
    "                col_name = col + '=' + str(g)\n",
    "                df[col_name] = df[col].values == g\n",
    "            df = df.drop(col, axis = 1)\n",
    "    return df, ft, ar, tox\n",
    "\n",
    "def test_classifiers(classifiers, \n",
    "                     db = None, \n",
    "                     log = False,\n",
    "                     feature_params = {},\n",
    "                     regularizer = QuantileTransformer(),\n",
    "                     data_splits = None,\n",
    "                     print_importances = False,\n",
    "                    additional_features = None):\n",
    "\n",
    "    result_template = {'cluster_names': copy(str(feature_params['cluster_names'])),\n",
    "                       'Baseline': str(feature_params['use_baseline_features']),\n",
    "                       'Top_features': str(feature_params['use_top_features']),\n",
    "                       'Top_feature_file': str(feature_params['top_features']),\n",
    "                      }\n",
    "\n",
    "    if log:\n",
    "        timestamp = datetime.fromtimestamp(time()).strftime('%Y_%m_%d_%H%M%S')\n",
    "        f = open(Constants.toxicity_log_file_root + timestamp +'.txt', 'w', buffering = 1)\n",
    "        def write(string):\n",
    "            print(string)\n",
    "            f.write(str(string)+'\\n')\n",
    "    else:\n",
    "        write = lambda string: print(string)\n",
    "        \n",
    "    df, ft, ar, tox = cluster_features(db, **feature_params)\n",
    "    if additional_features is not None:\n",
    "        #should be tuple of attributes, organ_list (default none) to pass to patientset.to_dataframe\n",
    "        df = db.to_dataframe(additional_features[0], df, additional_features[1])\n",
    "    write(str(feature_params))\n",
    "    outcomes = [(ft, 'feeding_tube'), (ar, 'aspiration'), (tox, 'toxicity')]\n",
    "    data_splits = get_all_splits(df, regularizer, outcomes) if data_splits is None else data_splits\n",
    "    print('splits finished')\n",
    "    results = []\n",
    "    for classifier in classifiers:\n",
    "        write(classifier)\n",
    "        for outcome in outcomes:\n",
    "            data_split = data_splits[outcome[1]]\n",
    "            for resampler_name, splits in data_split.items():\n",
    "                try:\n",
    "                    write(resampler_name)\n",
    "                    auc, importances = presplit_roc_cv(classifier, splits)\n",
    "                    write(outcome[1])\n",
    "                    write(auc)\n",
    "                    if importances is not None and print_importances:\n",
    "                        write(importances)\n",
    "                    write('\\n')\n",
    "                    result = copy(result_template)\n",
    "                    result['classifier'] = str(classifier)\n",
    "                    result['outcome'] = str(outcome[1])\n",
    "                    result['resampler'] = str(resampler_name)\n",
    "                    result['AUC'] = auc\n",
    "                    results.append(result)\n",
    "                except Exception as e:\n",
    "                    print(e)\n",
    "    if log:\n",
    "        f.close()\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "F:\\Skool\\EVL_Research\\CAMP-RT\\PYTHON\\Patient.py:359: RuntimeWarning: invalid value encountered in true_divide\n",
      "  mean_tumor_distances /= tumor_volume\n",
      "F:\\Skool\\EVL_Research\\CAMP-RT\\PYTHON\\Patient.py:360: RuntimeWarning: invalid value encountered in true_divide\n",
      "  tumor_position /= tumor_volume\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 [{0, 1}, {2}]\n",
      "128 [{0}, {1, 2, 3, 4}]\n",
      "notation not accounted for in lymph nodes: R3/R4\n",
      "notation not accounted for in lymph nodes: L2/3\n",
      "notation not accounted for in lymph nodes: R3/4\n",
      "notation not accounted for in lymph nodes: R2/3\n",
      "notation not accounted for in lymph nodes: R2-R4\n",
      "notation not accounted for in lymph nodes: L2/3\n",
      "notation not accounted for in lymph nodes: L2/3\n",
      "notation not accounted for in lymph nodes: L2/3\n",
      "notation not accounted for in lymph nodes: R2/3\n",
      "notation not accounted for in lymph nodes: R2/3/4\n",
      "notation not accounted for in lymph nodes: L2/3\n",
      "notation not accounted for in lymph nodes: L2/3\n",
      "notation not accounted for in lymph nodes: R2/3\n",
      "notation not accounted for in lymph nodes: R2/3\n",
      "notation not accounted for in lymph nodes: R2/3\n",
      "10021 [{0, 1}, {2}]\n",
      "error reading tumor volume for  10041\n",
      "10074 [{0, 1}, {2}]\n",
      "error reading tumor volume for  10091\n",
      "error reading tumor volume for  10148\n",
      "10191 [{0, 1}, {2}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0925 18:11:57.659312  3688 deprecation_wrapper.py:119] From F:\\Anaconda\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "W0925 18:11:57.661313  3688 deprecation_wrapper.py:119] From F:\\Anaconda\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "W0925 18:11:57.666313  3688 deprecation_wrapper.py:119] From F:\\Anaconda\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:133: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
      "\n",
      "W0925 18:11:57.675313  3688 deprecation_wrapper.py:119] From F:\\Anaconda\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:4115: The name tf.random_normal is deprecated. Please use tf.random.normal instead.\n",
      "\n",
      "W0925 18:11:57.824322  3688 deprecation_wrapper.py:119] From F:\\Anaconda\\lib\\site-packages\\keras\\optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "patient data loaded...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def augmented_db(db = None, db_args = {}):\n",
    "    if db is None:\n",
    "        db = PatientSet(**db_args)\n",
    "    db.toxicity = db.feeding_tubes + db.aspiration > 0\n",
    "    return(db)\n",
    "\n",
    "db = augmented_db()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifiers = [\n",
    "#                    DecisionTreeClassifier(),\n",
    "#                    DecisionTreeClassifier(criterion='entropy'),\n",
    "#                    XGBClassifier(1, booster = 'gblinear'),\n",
    "#                    XGBClassifier(3, booster = 'gblinear'),\n",
    "#                    XGBClassifier(5, booster = 'gblinear'),\n",
    "#                    XGBClassifier(),\n",
    "#                    XGBClassifier(booster = 'dart'),\n",
    "                    LogisticRegression(C = 1, solver = 'lbfgs', max_iter = 3000),\n",
    "#                    MetricLearningClassifier(use_softmax = True),\n",
    "#                    MetricLearningClassifier(\n",
    "#                            resampler = under_sampling.OneSidedSelection()),\n",
    "#                    MetricLearningClassifier(\n",
    "#                            resampler = under_sampling.CondensedNearestNeighbour()),\n",
    "#                    ExtraTreesClassifier(n_estimators = 200),\n",
    "#                    RandomForestClassifier(n_estimators = 200, max_depth = 3),\n",
    "#                    BayesWrapper(),\n",
    "                   ]\n",
    "\n",
    "resamplers = [\n",
    "                  None,\n",
    "#                  under_sampling.RandomUnderSampler(),\n",
    "#                  over_sampling.RandomOverSampler(),\n",
    "#                  under_sampling.InstanceHardnessThreshold(\n",
    "#                          estimator = MetricLearningClassifier(),\n",
    "#                          cv = 18),\n",
    "#                  under_sampling.InstanceHardnessThreshold(cv = 18),\n",
    "#                  over_sampling.SMOTE(),\n",
    "#                  combine.SMOTEENN(),\n",
    "#                  combine.SMOTETomek(),\n",
    "#                  under_sampling.InstanceHardnessThreshold(),\n",
    "#                  under_sampling.RepeatedEditedNearestNeighbours(),\n",
    "#                  under_sampling.EditedNearestNeighbours(),\n",
    "#                  under_sampling.CondensedNearestNeighbour(),\n",
    "#                  under_sampling.OneSidedSelection(),\n",
    "                  ]\n",
    "\n",
    "cluster_root= 'data/clustering_results/'\n",
    "feature_file_names = ['toxicityClustering', 'aspirationClustering', \n",
    "                      'feedingTubeClustering','toxicityClusteringWithNonspatial',\n",
    "                      'combinedClustering', 'combinedClusteringWithNonspatial']\n",
    "feature_file_names = ['combinedClusteringWithNonspatial']\n",
    "#llop through all files of feature extracted from toxicity clustering\n",
    "all_results = []\n",
    "additional_features = None\n",
    "\n",
    "run = lambda x: test_classifiers(classifiers, db, \n",
    "                                 log = True, \n",
    "                                 feature_params = x, \n",
    "                                 additional_features = additional_features)\n",
    "do_test = lambda: all_results.extend(run(feature_params))\n",
    "\n",
    "def try_features():\n",
    "    for feature_file in feature_file_names:\n",
    "        feature_params['top_features'] = cluster_root + feature_file + '.csv'\n",
    "        do_test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "baseline\n",
      "feeding_tubes\n",
      "0.7258682328907048\n",
      "aspiration\n",
      "0.7525443442861297\n",
      "toxicity\n",
      "0.7402551381998583\n",
      "\n",
      "combinedClusteringWithNonspatial\n",
      "feeding_tubes\n",
      "0.7589376915219612\n",
      "aspiration\n",
      "0.8569351555684792\n",
      "toxicity\n",
      "0.8038625088589653\n",
      "\n",
      "combinedClusteringWithNonspatial+baseline\n",
      "feeding_tubes\n",
      "0.7886874361593463\n",
      "aspiration\n",
      "0.8828147717359698\n",
      "toxicity\n",
      "0.8416017009213324\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#the the unsupervised AUC scores\n",
    "def print_cluster_results(clusters, name):\n",
    "    print(name)\n",
    "    for outcome in ['feeding_tubes', 'aspiration','toxicity']:\n",
    "        print(outcome)\n",
    "        tox = getattr(db, outcome)\n",
    "        pred_outcome = BayesWrapper().fit_predict(clusters, tox.reshape(-1,1), True)[:,1]\n",
    "        print(roc_auc_score(tox, pred_outcome))\n",
    "    print()\n",
    "    \n",
    "baseline_clusters = pd.read_csv(baseline_feature_file, index_col='Dummy.ID')['manhattan_k=4'].values.reshape(-1,1)\n",
    "print_cluster_results(baseline_clusters, 'baseline')\n",
    "\n",
    "for feature_file in feature_file_names:\n",
    "    file = cluster_root + feature_file + '.csv'\n",
    "    spatial_clusters = pd.read_csv(file).cluster_labels.values.reshape(-1,1)\n",
    "    print_cluster_results(spatial_clusters, feature_file)\n",
    "    both_clusters = np.hstack([spatial_clusters, baseline_clusters])\n",
    "    print_cluster_results(both_clusters, feature_file + '+baseline')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'use_baseline_features': False, 'top_features': None, 'use_top_features': False, 'discrete_features': False, 'cluster_names': ['manhattan_k=4']}\n",
      "splits finished\n",
      "LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,\n",
      "                   intercept_scaling=1, l1_ratio=None, max_iter=3000,\n",
      "                   multi_class='warn', n_jobs=None, penalty='l2',\n",
      "                   random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
      "                   warm_start=False)\n",
      "None\n",
      "feeding_tube\n",
      "0.5865679264555669\n",
      "\n",
      "\n",
      "None\n",
      "aspiration\n",
      "0.49723756906077343\n",
      "\n",
      "\n",
      "None\n",
      "toxicity\n",
      "0.6031183557760453\n",
      "\n",
      "\n",
      "{'use_baseline_features': False, 'top_features': 'data/clustering_results/combinedClusteringWithNonspatial.csv', 'use_top_features': False, 'discrete_features': False, 'cluster_names': ['manhattan_k=4', 'cluster_labels']}\n",
      "splits finished\n",
      "LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,\n",
      "                   intercept_scaling=1, l1_ratio=None, max_iter=3000,\n",
      "                   multi_class='warn', n_jobs=None, penalty='l2',\n",
      "                   random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
      "                   warm_start=False)\n",
      "None\n",
      "feeding_tube\n",
      "0.6917773237997957\n",
      "\n",
      "\n",
      "None\n",
      "aspiration\n",
      "0.7979063681302705\n",
      "\n",
      "\n",
      "None\n",
      "toxicity\n",
      "0.7703756201275691\n",
      "\n",
      "\n",
      "{'use_baseline_features': False, 'top_features': 'data/clustering_results/combinedClusteringWithNonspatial.csv', 'use_top_features': False, 'discrete_features': False, 'cluster_names': ['cluster_labels']}\n",
      "splits finished\n",
      "LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,\n",
      "                   intercept_scaling=1, l1_ratio=None, max_iter=3000,\n",
      "                   multi_class='warn', n_jobs=None, penalty='l2',\n",
      "                   random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
      "                   warm_start=False)\n",
      "None\n",
      "feeding_tube\n",
      "0.5965270684371808\n",
      "\n",
      "\n",
      "None\n",
      "aspiration\n",
      "0.758359988368712\n",
      "\n",
      "\n",
      "None\n",
      "toxicity\n",
      "0.6683203401842663\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#try out combinations of just clusters\n",
    "feature_params = {\n",
    "    'use_baseline_features': False,\n",
    "    'top_features': None,\n",
    "    'use_top_features': False,       \n",
    "    'discrete_features': False,\n",
    "    'cluster_names': ['manhattan_k=4']\n",
    "}\n",
    "do_test()\n",
    "for cluster_combo in [['manhattan_k=4', 'cluster_labels'],['cluster_labels']]:\n",
    "    feature_params['cluster_names'] = cluster_combo\n",
    "    try_features()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'use_baseline_features': True, 'top_features': 'data/clustering_results/toxicityClustering.csv', 'use_top_features': False, 'discrete_features': False, 'cluster_names': []}\n",
      "splits finished\n",
      "LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,\n",
      "                   intercept_scaling=1, l1_ratio=None, max_iter=3000,\n",
      "                   multi_class='warn', n_jobs=None, penalty='l2',\n",
      "                   random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
      "                   warm_start=False)\n",
      "None\n",
      "feeding_tube\n",
      "0.6404494382022471\n",
      "\n",
      "\n",
      "None\n",
      "aspiration\n",
      "0.8493748182611224\n",
      "\n",
      "\n",
      "None\n",
      "toxicity\n",
      "0.7888022678951098\n",
      "\n",
      "\n",
      "{'use_baseline_features': True, 'top_features': 'data/clustering_results/toxicityClustering.csv', 'use_top_features': False, 'discrete_features': False, 'cluster_names': ['manhattan_k=4']}\n",
      "splits finished\n",
      "LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,\n",
      "                   intercept_scaling=1, l1_ratio=None, max_iter=3000,\n",
      "                   multi_class='warn', n_jobs=None, penalty='l2',\n",
      "                   random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
      "                   warm_start=False)\n",
      "None\n",
      "feeding_tube\n",
      "0.6350868232890705\n",
      "\n",
      "\n",
      "None\n",
      "aspiration\n",
      "0.8470485606280895\n",
      "\n",
      "\n",
      "None\n",
      "toxicity\n",
      "0.7854358610914246\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#baseline test\n",
    "feature_params['use_baseline_features'] = True\n",
    "for cluster_combo in [[],['manhattan_k=4']]:\n",
    "    feature_params['cluster_names'] = cluster_combo\n",
    "    do_test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'use_baseline_features': True, 'top_features': 'data/clustering_results/combinedClusteringWithNonspatial.csv', 'use_top_features': False, 'discrete_features': False, 'cluster_names': ['manhattan_k=4', 'cluster_labels']}\n",
      "splits finished\n",
      "LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,\n",
      "                   intercept_scaling=1, l1_ratio=None, max_iter=3000,\n",
      "                   multi_class='warn', n_jobs=None, penalty='l2',\n",
      "                   random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
      "                   warm_start=False)\n",
      "None\n",
      "feeding_tube\n",
      "0.6350868232890705\n",
      "\n",
      "\n",
      "None\n",
      "aspiration\n",
      "0.8493748182611225\n",
      "\n",
      "\n",
      "None\n",
      "toxicity\n",
      "0.7834868887313962\n",
      "\n",
      "\n",
      "{'use_baseline_features': True, 'top_features': 'data/clustering_results/combinedClusteringWithNonspatial.csv', 'use_top_features': False, 'discrete_features': False, 'cluster_names': ['cluster_labels']}\n",
      "splits finished\n",
      "LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,\n",
      "                   intercept_scaling=1, l1_ratio=None, max_iter=3000,\n",
      "                   multi_class='warn', n_jobs=None, penalty='l2',\n",
      "                   random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
      "                   warm_start=False)\n",
      "None\n",
      "feeding_tube\n",
      "0.6419816138917263\n",
      "\n",
      "\n",
      "None\n",
      "aspiration\n",
      "0.8514102936900262\n",
      "\n",
      "\n",
      "None\n",
      "toxicity\n",
      "0.78756201275691\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#test out adding clusters from each type (baseline + clusters)\n",
    "feature_params['use_baseline_features'] = True\n",
    "feature_params['use_top_features'] = False\n",
    "feature_params['cluster_names'] = ['manhattan_k=4', 'cluster_labels']\n",
    "try_features()\n",
    "\n",
    "feature_params['cluster_names'] = ['cluster_labels']\n",
    "try_features()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'use_baseline_features': False, 'top_features': 'data/clustering_results/combinedClusteringWithNonspatial.csv', 'use_top_features': True, 'discrete_features': False, 'cluster_names': ['cluster_labels']}\n",
      "splits finished\n",
      "LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,\n",
      "                   intercept_scaling=1, l1_ratio=None, max_iter=3000,\n",
      "                   multi_class='warn', n_jobs=None, penalty='l2',\n",
      "                   random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
      "                   warm_start=False)\n",
      "None\n",
      "feeding_tube\n",
      "0.7456588355464759\n",
      "\n",
      "\n",
      "None\n",
      "aspiration\n",
      "0.8371619656876998\n",
      "\n",
      "\n",
      "None\n",
      "toxicity\n",
      "0.8125442948263643\n",
      "\n",
      "\n",
      "{'use_baseline_features': True, 'top_features': 'data/clustering_results/combinedClusteringWithNonspatial.csv', 'use_top_features': True, 'discrete_features': False, 'cluster_names': ['cluster_labels']}\n",
      "splits finished\n",
      "LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,\n",
      "                   intercept_scaling=1, l1_ratio=None, max_iter=3000,\n",
      "                   multi_class='warn', n_jobs=None, penalty='l2',\n",
      "                   random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
      "                   warm_start=False)\n",
      "None\n",
      "feeding_tube\n",
      "0.65755873340143\n",
      "\n",
      "\n",
      "None\n",
      "aspiration\n",
      "0.8540273335271881\n",
      "\n",
      "\n",
      "None\n",
      "toxicity\n",
      "0.7944720056697379\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#now try just spatial features\n",
    "feature_params['use_top_features'] = True\n",
    "feature_params['use_baseline_features'] = False\n",
    "feature_params['cluster_names']= ['cluster_labels']\n",
    "try_features()\n",
    "\n",
    "feature_params['use_baseline_features'] = True\n",
    "try_features()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save it all\n",
    "df = pd.DataFrame(all_results).sort_values(\n",
    "        ['classifier',\n",
    "         'outcome',\n",
    "         'AUC',\n",
    "         'resampler',\n",
    "         'cluster_names',\n",
    "         'Baseline'],\n",
    "         kind = 'mergesort',\n",
    "         ascending = False)\n",
    "df.to_csv('data/toxcity_classification_tests_'\n",
    "          + datetime.fromtimestamp(time()).strftime('%Y_%m_%d_%H%M%S')\n",
    "          + '.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
