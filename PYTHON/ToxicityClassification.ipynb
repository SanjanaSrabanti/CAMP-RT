{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy.random import seed\n",
    "seed(1)\n",
    "\n",
    "from PatientSet import PatientSet\n",
    "from Constants import Constants\n",
    "import Metrics\n",
    "from analysis import *\n",
    "from Models import *\n",
    "from copy import copy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from time import time\n",
    "from datetime import datetime\n",
    "\n",
    "from dependencies.NCA import NeighborhoodComponentsAnalysis\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "from scipy.special import softmax\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.preprocessing import OneHotEncoder, QuantileTransformer\n",
    "\n",
    "from sklearn.naive_bayes import BernoulliNB, ComplementNB, GaussianNB, MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "from sklearn.ensemble import VotingClassifier, ExtraTreesClassifier, RandomForestClassifier, AdaBoostClassifier\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "from imblearn import under_sampling, over_sampling, combine\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MetricLearningClassifier(BaseEstimator, ClassifierMixin):\n",
    "\n",
    "    def __init__(self, n_components = 'auto',\n",
    "                 random_state = 1,\n",
    "                 resampler = None,\n",
    "                 use_softmax = True):\n",
    "        self.n_components = n_components\n",
    "        if n_components is not 'auto':\n",
    "            self.transformer = NeighborhoodComponentsAnalysis(n_components = n_components)\n",
    "        self.group_parameters = namedtuple('group_parameters', ['means', 'inv_covariance', 'max_dist'])\n",
    "        self.resampler = resampler\n",
    "        self.use_softmax = use_softmax\n",
    "\n",
    "    def get_optimal_components(self, x, y):\n",
    "        n_components = x.shape[1]\n",
    "        def get_score():\n",
    "            nca = NeighborhoodComponentsAnalysis(n_components = n_components)\n",
    "            nca.fit(x,y)\n",
    "            return silhouette_score(nca.transform(x), y), nca\n",
    "        score, nca = get_score()\n",
    "        while True:\n",
    "            if n_components <= 2:\n",
    "                return nca\n",
    "            n_components -= 1\n",
    "            new_score, new_nca = get_score()\n",
    "            if new_score > 1.1*score:\n",
    "                score = new_score\n",
    "                nca = new_nca\n",
    "            else:\n",
    "                return nca\n",
    "\n",
    "    def fit(self, x, y):\n",
    "        if self.n_components == 'auto':\n",
    "            self.transformer = self.get_optimal_components(x, y)\n",
    "        self.transformer.fit(x, y)\n",
    "        self.groups = OrderedDict()\n",
    "        if self.resampler is not None:\n",
    "            xtemp, ytemp = self.resampler.fit_resample(x,y)\n",
    "            if len(np.unique(ytemp)) == len(np.unique(y)):\n",
    "                x = xtemp\n",
    "                y = ytemp\n",
    "        for group in np.unique(y):\n",
    "            self.groups[group] = self.group_params(x, y, group)\n",
    "\n",
    "    def group_params(self, x, y, group):\n",
    "        targets = np.argwhere(y == group).ravel()\n",
    "        x_target = self.transformer.transform(x[targets])\n",
    "        fmeans = x_target.mean(axis = 0)\n",
    "        inv_cov = np.linalg.pinv(np.cov(x_target.T))\n",
    "        train_dists = self.mahalanobis_distances(x, self.group_parameters(fmeans, inv_cov, 0))\n",
    "        parameters = self.group_parameters(fmeans, inv_cov, train_dists.max())\n",
    "        return parameters\n",
    "\n",
    "    def mahalanobis_distances(self, x, group):\n",
    "        x_offset = self.transformer.transform(x) - group.means\n",
    "        left_term = np.dot(x_offset, group.inv_covariance)\n",
    "        mahalanobis = np.dot(left_term, x_offset.T).diagonal()\n",
    "        return mahalanobis\n",
    "\n",
    "    def predict_proba(self, x):\n",
    "        all_distances = []\n",
    "        for group_id, group_params in self.groups.items():\n",
    "            distances = self.mahalanobis_distances(x, group_params)\n",
    "            proximity = np.clip(1 - (distances/group_params.max_dist), 0.00001, 1)\n",
    "            all_distances.append(proximity)\n",
    "        output = np.hstack(all_distances).reshape(-1, len(self.groups.keys()))\n",
    "        if self.use_softmax:\n",
    "            output = softmax(output)\n",
    "        else:\n",
    "            output = output/output.sum(axis = 1).reshape(-1,1)\n",
    "        return output\n",
    "\n",
    "    def predict(self, x):\n",
    "        labels = list(self.groups.keys())\n",
    "        probs = self.predict_proba(self, x)\n",
    "        max_probs =  np.argmax(probs, axis = 1).ravel()\n",
    "        ypred = np.zeros(max_probs.shape).astype(np.dtype(labels[0]))\n",
    "        for i in range(max_probs.shape[0]):\n",
    "            ypred[i] = labels[max_probs[i]]\n",
    "        return ypred[i]\n",
    "\n",
    "    def fit_predict(self, x, y):\n",
    "        self.fit(x,y)\n",
    "        return self.predict(x)\n",
    "\n",
    "class BayesWrapper(BaseEstimator, ClassifierMixin):\n",
    "\n",
    "    def __init__(self, bayes = BernoulliNB(alpha = 0), n_categories = None):\n",
    "        if n_categories is None:\n",
    "            self.encoder = OneHotEncoder(categories = 'auto',\n",
    "                                         sparse = False,\n",
    "                                         handle_unknown = 'ignore')\n",
    "        else:\n",
    "            self.encoder = KBinsDiscretizer(n_bins = n_categories, encode = 'ordinal')\n",
    "        self.bayes = bayes\n",
    "\n",
    "    def fit(self, x, y):\n",
    "        x = self.encoder.fit_transform(x)\n",
    "        self.bayes.fit(x,y)\n",
    "        return self\n",
    "\n",
    "    def predict(self, x):\n",
    "        xpred = self.encoder.transform(x)\n",
    "        return self.bayes.predict(xpred)\n",
    "\n",
    "    def predict_proba(self, x):\n",
    "        xpred = self.encoder.transform(x)\n",
    "        return self.bayes.predict_proba(xpred)\n",
    "\n",
    "    def fit_predict(self, x, y):\n",
    "        self.fit(x,y)\n",
    "        return self.predict(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discretize_continuous_fields(df, n_bins):\n",
    "    encoder = KBinsDiscretizer(n_bins = n_bins, encode = 'ordinal')\n",
    "    for col in df.columns:\n",
    "        vals = df[col].values\n",
    "        if len(np.unique(vals)) > n_bins:\n",
    "            df[col] = encoder.fit_transform(vals.reshape(-1,1)).ravel()\n",
    "    return df\n",
    "\n",
    "def get_all_splits(df, regularizer, outcomes, resamplers = None):\n",
    "    if resamplers is None:\n",
    "        resamplers = [None,\n",
    "                  under_sampling.RandomUnderSampler(),\n",
    "                  over_sampling.RandomOverSampler(),\n",
    "#                  under_sampling.InstanceHardnessThreshold(\n",
    "#                          estimator = MetricLearningClassifier(),\n",
    "#                          cv = 18),\n",
    "                  under_sampling.InstanceHardnessThreshold(cv = 18),\n",
    "                  over_sampling.SMOTE(),\n",
    "                  combine.SMOTEENN(),\n",
    "                  combine.SMOTETomek(),\n",
    "                  under_sampling.InstanceHardnessThreshold(),\n",
    "                  under_sampling.RepeatedEditedNearestNeighbours(),\n",
    "                  under_sampling.EditedNearestNeighbours(),\n",
    "                  under_sampling.CondensedNearestNeighbour(),\n",
    "                  under_sampling.OneSidedSelection(),\n",
    "                  ]\n",
    "    data_splits = {}\n",
    "    for outcome in outcomes:\n",
    "        splits = {str(resampler): get_splits(df, outcome[0], regularizer, [resampler]) for resampler in resamplers}\n",
    "        data_splits[outcome[1]] = splits\n",
    "    return data_splits\n",
    "\n",
    "def get_splits(df, y, regularizer = None, resamplers = None):\n",
    "    x = df.values\n",
    "    feature_labels = list(df.columns)\n",
    "    loo = LeaveOneOut()\n",
    "    splits = []\n",
    "    for train, test in loo.split(x):\n",
    "        split = {}\n",
    "        xtrain, ytrain = x[train], y[train]\n",
    "        xtest, ytest = x[test], y[test]\n",
    "        if regularizer is not None:\n",
    "            xtrain = regularizer.fit_transform(xtrain)\n",
    "            xtest = regularizer.transform(xtest)\n",
    "        for resampler in resamplers:\n",
    "            if resampler is None:\n",
    "                continue\n",
    "            xtrain, ytrain = resampler.fit_resample(xtrain, ytrain)\n",
    "        split['xtrain'] = xtrain\n",
    "        split['xtest'] = xtest\n",
    "        split['ytrain'] = ytrain\n",
    "        split['ytest'] = ytest\n",
    "        split['train_index'] = train\n",
    "        split['test_index'] = test\n",
    "        split['feature_labels'] = feature_labels\n",
    "        splits.append(split)\n",
    "    return splits\n",
    "\n",
    "def cluster_features(db,\n",
    "                  baseline_features = 'data/baselineClustering.csv',\n",
    "                  use_baseline_features = True,\n",
    "                  top_features = 'data/clustering_results/toxicityClustering.csv',\n",
    "                  use_top_features = True,\n",
    "                  discrete_features = False,\n",
    "                  cluster_names = ['kmeans_k=4','cluster_labels']):\n",
    "    baseline = pd.read_csv(baseline_features, index_col = 'Dummy.ID').drop('Unnamed: 0', axis = 1)\n",
    "    all_clusters = set(['manhattan_k=2','manhattan_k=3','manhattan_k=4',\n",
    "                        'cluster_labels','hc_ward2','hc_ward4',\n",
    "                        'FT','AR','TOX'])\n",
    "    non_features = list(all_clusters - set(cluster_names))\n",
    "    \n",
    "    if use_baseline_features:\n",
    "        cluster_names = cluster_names + list(baseline.drop(non_features, axis = 1, errors='ignore').columns)\n",
    "    if 'T.category' in cluster_names:\n",
    "        dist_clusters['T.category'] = dist_clusters['T.category'].apply(lambda x: int(x[1]))\n",
    "        \n",
    "    if isinstance(top_features, str):\n",
    "        dist_clusters = pd.read_csv(top_features, index_col = 0)\n",
    "        dist_clusters.index.rename('Dummy.ID', inplace = True)\n",
    "        if use_top_features:\n",
    "            cluster_names = cluster_names + list( dist_clusters.drop(non_features,axis=1, errors='ignore').columns)\n",
    "        df = baseline.merge(dist_clusters, on=['Dummy.ID'])\n",
    "    else:\n",
    "        df = baseline\n",
    "    ft = df.FT.values\n",
    "    ar = df.AR.values\n",
    "    tox = df.TOX.values\n",
    "    to_drop = set(df.columns) - set(cluster_names)\n",
    "    df = df.drop(to_drop, axis = 1, errors = 'ignore')\n",
    "    if discrete_features:\n",
    "        df = discretize_continuous_fields(df, 5)\n",
    "    columns = df.columns\n",
    "    for col in columns:\n",
    "        if col in all_clusters:\n",
    "            groups = set(df[col].values)\n",
    "            for g in groups:\n",
    "                col_name = col + '=' + str(g)\n",
    "                df[col_name] = df[col].values == g\n",
    "            df = df.drop(col, axis = 1)\n",
    "    print(df.columns)\n",
    "    return df, ft, ar, tox\n",
    "\n",
    "def test_classifiers(classifiers, \n",
    "                     db = None, \n",
    "                     log = False,\n",
    "                     feature_params = {},\n",
    "                     regularizer = QuantileTransformer(),\n",
    "                     data_splits = None,\n",
    "                     print_importances = False):\n",
    "\n",
    "    result_template = {'cluster_names': copy(str(feature_params['cluster_names'])),\n",
    "                       'Baseline': str(feature_params['use_baseline_features']),\n",
    "                       'Top_features': str(feature_params['use_top_features'])}\n",
    "\n",
    "    if log:\n",
    "        timestamp = datetime.fromtimestamp(time()).strftime('%Y_%m_%d_%H%M%S')\n",
    "        f = open(Constants.toxicity_log_file_root + timestamp +'.txt', 'w', buffering = 1)\n",
    "        def write(string):\n",
    "            print(string)\n",
    "            f.write(str(string)+'\\n')\n",
    "    else:\n",
    "        write = lambda string: print(string)\n",
    "        \n",
    "    df, ft, ar, tox = cluster_features(db, **feature_params)\n",
    "\n",
    "    write('features: ' + ', '.join([str(c) for c in df.columns]) + '\\n')\n",
    "    outcomes = [(ft, 'feeding_tube'), (ar, 'aspiration')]\n",
    "    data_splits = get_all_splits(df, regularizer, outcomes) if data_splits is None else data_splits\n",
    "    print('splits finished')\n",
    "    results = []\n",
    "    for classifier in classifiers:\n",
    "        write(classifier)\n",
    "        for outcome in outcomes:\n",
    "            data_split = data_splits[outcome[1]]\n",
    "            for resampler_name, splits in data_split.items():\n",
    "                try:\n",
    "                    write(resampler_name)\n",
    "                    auc, importances = presplit_roc_cv(classifier, splits)\n",
    "                    write(outcome[1])\n",
    "                    write(auc)\n",
    "                    if importances is not None:\n",
    "                        write(importances)\n",
    "                    write('\\n')\n",
    "                    result = copy(result_template)\n",
    "                    result['classifier'] = str(classifier)\n",
    "                    result['outcome'] = str(outcome[1])\n",
    "                    result['resampler'] = str(resampler_name)\n",
    "                    result['AUC'] = auc\n",
    "                    results.append(result)\n",
    "                except:\n",
    "                    continue\n",
    "    if log:\n",
    "        f.close()\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\git_repos\\CAMP-RT\\PYTHON\\Patient.py:359: RuntimeWarning: invalid value encountered in true_divide\n",
      "  mean_tumor_distances /= tumor_volume\n",
      "D:\\git_repos\\CAMP-RT\\PYTHON\\Patient.py:360: RuntimeWarning: invalid value encountered in true_divide\n",
      "  tumor_position /= tumor_volume\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 [{0, 1}, {2}]\n",
      "128 [{0}, {1, 2, 3, 4}]\n",
      "notation not accounted for in lymph nodes: R3/R4\n",
      "notation not accounted for in lymph nodes: L2/3\n",
      "notation not accounted for in lymph nodes: R3/4\n",
      "notation not accounted for in lymph nodes: R2/3\n",
      "notation not accounted for in lymph nodes: R2-R4\n",
      "notation not accounted for in lymph nodes: L2/3\n",
      "notation not accounted for in lymph nodes: L2/3\n",
      "notation not accounted for in lymph nodes: L2/3\n",
      "notation not accounted for in lymph nodes: R2/3\n",
      "notation not accounted for in lymph nodes: R2/3/4\n",
      "notation not accounted for in lymph nodes: L2/3\n",
      "notation not accounted for in lymph nodes: L2/3\n",
      "notation not accounted for in lymph nodes: R2/3\n",
      "notation not accounted for in lymph nodes: R2/3\n",
      "notation not accounted for in lymph nodes: R2/3\n",
      "10021 [{0, 1}, {2}]\n",
      "10074 [{0, 1}, {2}]\n",
      "error reading tumor volume for  10091\n",
      "error reading tumor volume for  10148\n",
      "10191 [{0, 1}, {2}]\n",
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\control_flow_ops.py:423: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "\n",
      "patient data loaded...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def augmented_db(db = None, db_args = {}):\n",
    "    if db is None:\n",
    "        db = PatientSet(**db_args)\n",
    "    db.discrete_dists = Metrics.discretize(-db.tumor_distances, n_bins = 15, strategy='uniform')\n",
    "    db.t_volumes = np.array([np.sum([g.volume for g in gtvs]) for gtvs in db.gtvs]).reshape(-1,1)\n",
    "    db.bilateral = db.lateralities == 'B'\n",
    "    db.pdoses = default_rt_prediction(db)\n",
    "    db.t4 = db.t_categories == 'T4'\n",
    "    db.m_volumes = db.volumes[:, [Constants.organ_list.index('Rt_Masseter_M'), Constants.organ_list.index('Lt_Masseter_M')]].sum(axis = 1).ravel()\n",
    "    return(db)\n",
    "\n",
    "db = augmented_db()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Age.at.Diagnosis..Calculated.', 'Total.dose', 'Pathological.Grade_II',\n",
      "       'Pathological.Grade_III', 'Pathological.Grade_IV',\n",
      "       'Pathological.Grade_I', 'Therapeutic.combination_CC',\n",
      "       'Therapeutic.combination_Radiation.alone',\n",
      "       'Therapeutic.combination_IC.CC',\n",
      "       'Therapeutic.combination_IC.Radiation.alone', 'Tm.Laterality..R.L._L',\n",
      "       'Tm.Laterality..R.L._R', 'HPV.P16.status_Positive',\n",
      "       'HPV.P16.status_Negative', 'T.category_T2', 'T.category_T1',\n",
      "       'T.category_T3', 'T.category_T4', 'N.category_8th_edition_N1',\n",
      "       'N.category_8th_edition_N2', 'N.category_8th_edition_N3',\n",
      "       'N.category_8th_edition_N0', 'AJCC.8th.edition_I',\n",
      "       'AJCC.8th.edition_II', 'AJCC.8th.edition_IV', 'AJCC.8th.edition_III',\n",
      "       'Smoking.status.at.Diagnosis..Never.Former.Current._Never',\n",
      "       'Smoking.status.at.Diagnosis..Never.Former.Current._Current',\n",
      "       'Smoking.status.at.Diagnosis..Never.Former.Current._Formar',\n",
      "       'Tumor.subsite_Tonsil', 'Tumor.subsite_BOT', 'Tumor.subsite_Other'],\n",
      "      dtype='object')\n",
      "features: Age.at.Diagnosis..Calculated., Total.dose, Pathological.Grade_II, Pathological.Grade_III, Pathological.Grade_IV, Pathological.Grade_I, Therapeutic.combination_CC, Therapeutic.combination_Radiation.alone, Therapeutic.combination_IC.CC, Therapeutic.combination_IC.Radiation.alone, Tm.Laterality..R.L._L, Tm.Laterality..R.L._R, HPV.P16.status_Positive, HPV.P16.status_Negative, T.category_T2, T.category_T1, T.category_T3, T.category_T4, N.category_8th_edition_N1, N.category_8th_edition_N2, N.category_8th_edition_N3, N.category_8th_edition_N0, AJCC.8th.edition_I, AJCC.8th.edition_II, AJCC.8th.edition_IV, AJCC.8th.edition_III, Smoking.status.at.Diagnosis..Never.Former.Current._Never, Smoking.status.at.Diagnosis..Never.Former.Current._Current, Smoking.status.at.Diagnosis..Never.Former.Current._Formar, Tumor.subsite_Tonsil, Tumor.subsite_BOT, Tumor.subsite_Other\n",
      "\n"
     ]
    }
   ],
   "source": [
    "classifiers = [\n",
    "#                    DecisionTreeClassifier(),\n",
    "#                    DecisionTreeClassifier(criterion='entropy'),\n",
    "#                    XGBClassifier(1, booster = 'gblinear'),\n",
    "#                    XGBClassifier(3, booster = 'gblinear'),\n",
    "#                    XGBClassifier(5, booster = 'gblinear'),\n",
    "#                    XGBClassifier(),\n",
    "#                    XGBClassifier(booster = 'dart'),\n",
    "                    LogisticRegression(C = 1, solver = 'lbfgs', max_iter = 3000),\n",
    "#                    MetricLearningClassifier(use_softmax = True),\n",
    "#                    MetricLearningClassifier(\n",
    "#                            resampler = under_sampling.OneSidedSelection()),\n",
    "#                    MetricLearningClassifier(\n",
    "#                            resampler = under_sampling.CondensedNearestNeighbour()),\n",
    "#                    ExtraTreesClassifier(n_estimators = 200),\n",
    "#                    RandomForestClassifier(n_estimators = 200, max_depth = 3),\n",
    "#                    BayesWrapper(),\n",
    "                   ]\n",
    "feature_params = {\n",
    "    'use_baseline_features': True,\n",
    "    'top_features': None,\n",
    "    'use_top_features': False,       \n",
    "    'discrete_features': False,\n",
    "    'cluster_names': ['kmeans_k=4','cluster_labels']\n",
    "}\n",
    "cluster_root= 'data/clustering_results/'\n",
    "all_results = []\n",
    "\n",
    "run = lambda x: test_classifiers(classifiers, db, log = True, feature_params = x)\n",
    "do_test = lambda: all_results.extend(run(feature_params))\n",
    "    \n",
    "for cluster_combo in [[],['manhattan_k=4'],['cluster_labels'],['manhattan_k=4','cluster_labels']]:\n",
    "    feature_params['cluster_names'] = cluster_combo\n",
    "    do_test()\n",
    "\n",
    "#test out extracted Features\n",
    "feature_params['use_top_features'] = True\n",
    "feature_params['cluster_names'] = ['cluster_labels']\n",
    "for feature_file in ['toxicity', 'aspiration', 'feedingTube']:\n",
    "    feature_params['top_features'] = cluster_root + feature_file + '.csv'\n",
    "    do_test()\n",
    "    \n",
    "feature_params['use_baseline_features'] = False\n",
    "for feature_file in ['toxicity', 'aspiration', 'feedingTube']:\n",
    "    feature_params['top_features'] = cluster_root + feature_file + '.csv'\n",
    "    do_test()\n",
    "    \n",
    "df = pd.DataFrame(all_results).sort_values(\n",
    "        ['classifier',\n",
    "         'outcome',\n",
    "         'AUC',\n",
    "         'resampler',\n",
    "         'cluster_names',\n",
    "         'Baseline'],\n",
    "         kind = 'mergesort',\n",
    "         ascending = False)\n",
    "df.to_csv('data/toxcity_classification_tests_'\n",
    "          + datetime.fromtimestamp(time()).strftime('%Y_%m_%d_%H%M%S')\n",
    "          + '.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
